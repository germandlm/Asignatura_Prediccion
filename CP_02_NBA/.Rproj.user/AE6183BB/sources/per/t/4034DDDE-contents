---
title: "CP 02 NBA"
author: "German de los Mozos"
date: "5/11/2020"
output: html_document
---

---

[//]: Comentario


## Reference

https://bradleyboehmke.github.io/HOML/engineering.html#proper-implementation

# Libraries and functions

```{r Libraries and functions, message=FALSE, warning=FALSE}
library(here) # Comentar
library(tidyverse)
library(janitor) # Limpiar los nombres
library(skimr) # Summarize bonito
library(magrittr) # Pipe operators
library(corrplot) # Correlaciones
library(ggcorrplot)  # Correlacion
library(PerformanceAnalytics) # Correlaciones
library(leaps) # Seleccion de modelo

```



# Read Data

```{r Read Data}
raw_data <-  read.csv("nba.csv")
colnames(raw_data)
```


# Variables Names

```{r}
raw_data %<>% clean_names()
colnames(raw_data)
```


# Summarize Data

```{r Summarise Data}

skim(raw_data)

```

* **Hay dos datos repetidos y varios NA**


# Data Wrangling data

* Data wrangling is the process of cleaning and unifying complex data sets for analysis, in turn boosting productivity within an organization.

```{r Data Wranling}
# delete duplicate
# Remove duplicate rows of the dataframe
raw_data %<>% distinct(player,.keep_all = TRUE)

# delete NA's
raw_data %<>% drop_na()

# Summarise
skim(raw_data)

```



```{r fig.height = 20, fig.width = 4, fig.align = "center"}

raw_data %>% 
  select_at(vars(-c("player","nba_country","tm"))) %>% 
  tidyr::gather("id", "value", 2:25) %>% 
  ggplot(., aes(y = salary, x = value)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "black") +
  facet_wrap(~id,ncol = 2,scales = "free_x")
```

```{r fig.height = 20, fig.width = 4, fig.align = "center"}

raw_data %>% 
  select_at(vars(-c("player","nba_country","tm"))) %>% # Seleccionar todo menos esas variables categoricas
  tidyr::gather("id", "value", 2:25) %>%  #Poner valores en columna
  ggplot(., aes(y = log(salary), x = value)) + # Usamos el logaritmo
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "black") +
  facet_wrap(~id,ncol = 2,scales = "free_x") # Utiliamos este ggplot para graficar los resultados por ID y categoria, con grafico de dispersion junto con regresion lineal
```

# EDA
## Log salary

```{r Log salary,fig.height = 10, fig.width = 10, fig.align = "center"}

log_data <- raw_data %>% mutate(salary = log(salary))

skim(log_data)
# Excluded vars (factor)

vars <- c("player","nba_country","tm")

# Correlations
corrplot(cor(log_data %>% 
               select_at(vars(-vars)), 
             use = "complete.obs"), 
         method = "circle",type = "upper")

# Other Correlations


ggcorrplot(cor(log_data %>% 
               select_at(vars(-vars)), 
            use = "complete.obs"),
            hc.order = TRUE,
            type = "lower",  lab = TRUE)


```


```{r fig.height = 20, fig.width =20, fig.align = "center"}

# Other Correlations

chart.Correlation(log_data %>% 
               select_at(vars(-vars)),
               histogram = TRUE, pch = 19)


```
```{r}
library(rsample)  # data splitting 
library(glmnet)   # implementing regularized regression approaches
library(dplyr)    # basic data manipulation procedures
```
``` {r}
set.seed(123)
nba_split <- initial_split(log_data, prop = .7, strata = "salary")
nba_train <- training(nba_split)
nba_test  <- testing(nba_split)
```
```{r}
nba_train_x <- model.matrix(salary ~ ., nba_train)[, -1]
nba_train_y <- log(nba_train$salary)

nba_test_x <- model.matrix(salary ~ ., nba_test)[, -1]
nba_test_y <- log(nba_test$salary)
dim(nba_train_x)
dim(nba_test_x)
```

``` {r}
fold_id <- sample(1:10, size = length(nba_train_y), replace=TRUE)

tuning_grid <- tibble::tibble(
  alpha      = seq(0, 1, by = .1),
  mse_min    = NA,
  mse_1se    = NA,
  lambda_min = NA,
  lambda_1se = NA
)
tuning_grid
```

```{r}
for (i in seq_along(tuning_grid$alpha)) {
  
  
  fit <- cv.glmnet(nba_train_x, nba_train_y, alpha = tuning_grid$alpha[i], foldid = fold_id)
  
 
  tuning_grid$mse_min[i]    <- fit$cvm[fit$lambda == fit$lambda.min]
  tuning_grid$mse_1se[i]    <- fit$cvm[fit$lambda == fit$lambda.1se]
  tuning_grid$lambda_min[i] <- fit$lambda.min
  tuning_grid$lambda_1se[i] <- fit$lambda.1se
}

tuning_grid
```
```{r}
tuning_grid %>%
  mutate(se = mse_1se - mse_min) %>%
  ggplot(aes(alpha, mse_min)) +
  geom_line(size = 2) +
  geom_ribbon(aes(ymax = mse_min + se, ymin = mse_min - se), alpha = .25) +
  ggtitle("MSE ± one standard error")
```
Calculo del error asociado al lasso con el training

```{r}
cv_lasso   <- cv.glmnet(nba_train_x, nba_train_y, alpha = 0.2)
min(cv_lasso$cvm)
```
Predicción y calculo del error con el dataset de test 

```{r}
pred <- predict(cv_lasso, s = cv_lasso$lambda.min, nba_test_x)
mean((nba_test_y - pred)^2)
```